\documentclass[12pt]{article}

\usepackage{url} % websites in bib
\usepackage{hyperref}
\usepackage[a4paper,left=1in,right=1in,top=1in,bottom=1in]{geometry} % margins

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{todonotes}
\usepackage{float} % figure placement
\usepackage[]{algorithm2e}

\begin{document}
	\title{A benchmark for histogram building algorithms of numeric streams}
	\author{Stefan Sebastian, 242}
	\date{}
	\maketitle
	
	\newpage
	\tableofcontents
	\newpage

	\section{Introduction}
	Main goals 
	Motivation 

	\subsection{Histograms}
	Short description of histograms use + why streaming 
	
	//todo reformulate, benahim2.1 A histogram is a set of B pairs (called bins) of real numbers {(p1,m1),...,(pB,mB)}, where B is a
	preset constant integer. The histogram is a compressed and approximate representation of a set S
	of real numbers


	Paper structure

	\section{Algorithms}
	\section{Numeric Histogram}
	\subsection{Overview}
	The Numeric Histogram algorithm first appeared in the paper by Ben-Haim and Tom-Tov
	on building a streaming parallel decision tree algorithm\cite{Ben-Haim:2010:SPD:1756006.1756034}.
	The main focus was to describe a method for tree based classifiers for large data 
	sets in a distributed environment, however they needed a data structure that 
	can summarize large amounts of data accurately. Thus, they proposed a histogram 
	data structure that can adapt to the requirements of a streaming environment. 

	The histogram maintains a fixed number of bins of the shape (p, m) where 
	p represents a central value of the interval and m the number of points in it.
	Initially the histogram has some bins allocated but the values are not known. 
	They are filled as data comes is added into the structure and once the allocated 
	number of bins is reached the central bin values are updated depending on incoming
	values. In this regard, the algorithm is robust to changes in data patterns over 
	a longer period of time, which is one of the main concerns of streaming analysis 
	algorithms.

	\subsection{Procedures}
	The proposed data structure contains four procedures: update, merge, sum and uniform.
	But for the purpose of this benchmark only two were needed: update and sum. The update 
	procedure adds a new point to the histogram data structure and is described in algorithm \ref{add}.

	\begin{algorithm}
		\label{add}
		\KwData{histogram $h = {(p_1, m_1), .., (p_b, m_b)}$, a point p}
		\KwResult{a histogram that represents the set $S \bigcup \{p\}$}
		binary search for the closest $p_i$ larger than p\;
		\eIf{$p_i = p$}{
			$m_i = m_i + 1$\;
		}{
			add a new bin of shape (p, 1) to the histogram at the i-th position\;
			find the closest two bins by their p values\;
			merge those bins, moving p proportional to their m values\;  
		}
		\caption{add procedure}
	\end{algorithm}

	The sum procedure, depicted in algorithm \ref{sum}, obtains the estimated number of points between $(-\infty, n]$ where n is 
	the input value. In order to estimate the number of values in a range $[a, b]$ we can calculate 
	$sum(b) - sum(a)$. The algorithm assumes that for each bin (p, m), there are $m/2$ points to the left of p
	and $m/2$ to the right. This means that the number of points in the interval $[p_i, i_{i+1}]$ is equal 
	to $(m_i + m_{i+1}) / 2$, which is the area of the trapezoid $(p_i, 0), (p_i, m_i), (p_{i+1}, m_{i+1}), (p_{i+1}, 0)$, 
	divided by $p_{i+1} - p_i$. Similarly, we can estimate the number of points in the 
	interval $[p_i, p]$ by adding calculating its projection on the line from $(p_i, m_i)$ to
	$(p_{i+1}, m_{i+1})$, then find the area of the new trapezoid and dividing again by $p_{i+1} - p_i$.
	The algorithm will not work properly if the point p is smaller than $p_0$ or larger than 
	$p_b$. For this reason the data structure should be initialized with a lower and upper bound
	into which all incoming data should fit.

	\begin{algorithm}
		\label{sum}
		\KwData{a histogram h, a point p such that $p_1 < p < p_b$}
		\KwResult{estimated number of points in the interval $[-\infty, p]$}

		binary search to find i such that $p_i <= p <= p_{i+1}$\;
		set $s = (m_i + m_p) \cdot (b - p_i) / 2 \cdot (p_{i+1} - p_i) $\;
		where $m_p = m_i + (m_{i+1} - m_i) \cdot (p - p_i) / (p_{i+1} - p_i)$\;

		\For{$j < i$}{
			$s = s + m_j$\;
		}
		$s = s + m_i / 2$\;
		\caption{sum procedure}
	\end{algorithm}

	\subsection{In practice}
	This algorithm was adapted and implemented into the open source project Apache 
	Hive\cite{Thusoo:2009:HWS:1687553.1687609}, which is a datawarehouse solution 
	built on top of Hadoop to provide data query and analysis methods. The project 
	is used actively in industry, handling reporting tasks for large volumes of data,
	for e.g. data produced by 435M monthly users of Chitika\cite{HivePoweredBy}.
	The version of the algorithm presented in this report is based on the Hive 
	implementation, called NumericHistogram\cite{HiveImplementation}. The optimal 
	number of bins is left as a choice to the user, however the suggested range 
	is between 20 and 80.


	\subsection{Optimal Streaming Histograms}
	\subsection{Overview}
	This algorithm was presented in a blog post\cite{OSHistograms} 
	by an engineer from the Amplitude company. Therefore, it was developed 
	with industry requirements in mind, and tested in realistic conditions 
	before being published. The main challenge was to avoid storing gigantic 
	amounts of streaming data and to uncover an optimal bucketing solution.
	The key requirements they identified for the bucket boundaries were:
	to be useful and reasonable for any range of data and to remain 
	useful upon changes in data distribution.
	
	The algorithm was also designed with data visualization needs in mind. 
	For this reason the buckets size and spacing have been thought out to 
	look intuitive in a chart and be easy to interpret.

	\subsection{Iterations}
	In order to reach a satisfying solution they went through multiple iterations 
	of the algorithm. The first one was to save the first 1000 distinct values 
	on the stream and then make evenly spaced buckets using them. However, this 
	method behaves poorly when distribution changes over time because the buckets 
	are fixed. Also, this might have resolution issues when the distribution 
	is skewed, for example: a lot of points in a small range of values. 
	
	The second technique was iterative merging of the closest values into buckets. 
	This was done in order to solve the resolution problem. Basically, among the 
	first 1000 values, the 2 closest ones are merged into a bucket. This process 
	is repeated until only 50 buckets are left. This improves resolution, however 
	it causes difficulties in interpreting the histograms as all buckets have 
	different widths and the spacing seems arbitrary. 

	\subsection{Algorithm}
	The final form of the algorithm is to pre-create buckets on a logarithmic 
	scale. This requires the user to know the boundaries of the incoming data, 
	and then, using those bounds, to create buckets that have 10\% increments 
	for every order of magnitude. For example, for the range 1 - 10, there will be 
	buckets of size 0.1, for the range 10 - 100, of size 1 and so forth. This solves
	the resolution issues because each value will be in a bucket in a 10\% range 
	of its true value. The spacing issues are also solved, and data visualization
	is easier and more intuitive. The fixed size of the buckets should not be a 
	problem in this scheme, which should work with a variety of data set types.

	\subsection{Implementation}
	The blog post did not provide an implementation so a variant will be 
	proposed in this report. The initialization step is done with a given 
	lower and upper bound. The smallest power of 10 larger than the upper bound 
	and the largest power of 10 smaller than the lower bound are found and 
	for each value p between them 90 buckets of width $p / 10$ are created.

	For the add procedure, the bucket is found using binary search which 
	provides a logarithmic complexity. The bin is the incremented by 1 
	for each addition. In order to estimate\ref{estimateOSH} the number of values in a range $ [a, b] $ 
	two binary searches are performed, for the bin of a and the bin of b. We 
	need to estimate the number of values in the range $[a, bUb]$, where bUb is the upper
	bound of the bucket which contains a, and in the range $[bLb, b]$ where bLb is the
	lower bound of the bucket which contains b. The approach is similar for both 
	intervals, so only one will be described. In order to estimate the number 
	of values in $[a, bUb]$, a ratio is calculated between the distance from 
	a to the upper bound and the size of the bucket. This ratio is then multiplied 
	with the number of values in the bucket. The values for the buckets contained 
	in the range $ [bUb, bLb] $ are added as they are.
	
	\begin{algorithm}
		\label{estimateOSH}
		\KwData{a histogram h, two points a, b}
		\KwResult{estimated number of points in the interval $[a, b]$}

		estimate = 0\;
		bin1 = getBin(a), bin2 = getBin(b)\;
		$lowerBound = bin1.val$, $upperBound = lowerBound \cdot 10$\;
		$ratio = (upperBound - a) / (upperBound - lowerBound)$\;
		$estimate = estimate + ratio \cdot bin1.count$\;

		\For{each bin from bin1 to bin2} {
			$estimate = estimate + bin.count$
		}

		$lowerBound = bin2.val$, $upperBound = lowerBound \cdot 10$\;
		$ratio = (b - lowerbound) / (upperBound - lowerBound)$\;
		$estimate = estimate + ratio \cdot bin2.count$\;

		\caption{Estimated number of points in range}
	\end{algorithm}

	\section{Benchmark}
	TODO describe benchmark methodology
	tests
	results

	\newpage
	\bibliography{references}
	\bibliographystyle{ieeetr}
\end{document}